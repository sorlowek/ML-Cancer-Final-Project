{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "from joblib import load\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.backend import manual_variable_initialization\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "preprocess_pipeline = load('../Model_Info/pipeline.joblib')\n",
    "import pickle\n",
    " \n",
    "from numpy.random import seed\n",
    "seed(1) #seed fixing for keras\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def NNpredict(values):\n",
    "    \n",
    "    values = values.reshape(1,-1)\n",
    "    values_scaled = preprocess_pipeline.transform(values)\n",
    "\n",
    "    model = load_model(\"../Model_Info/neuralnetwork.h5\")\n",
    "\n",
    "    prediction = model.predict(values_scaled)\n",
    "\n",
    "    NN_Dict = {\"Larynx\": f\"{prediction[0][0]*100}%\", \"Lung\": f\"{prediction[0][1]*100}%\",\n",
    "                \"Nasal\": f\"{prediction[0][2]*100}%\", \"Trachea\": f\"{prediction[0][3]*100}%\"}\n",
    "\n",
    "    \n",
    "    return NN_Dict  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNNvalues (state, race, year, gender):\n",
    "\n",
    "    \n",
    "\n",
    "    statelist = pd.Series(data=('AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI','IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME', 'MI','MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV','NY', 'OH', 'OK', 'OR', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN', 'TX','UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY'))\n",
    "    racelist =pd.Series(data=(\"American Indian or Alaska Native\", \"Asian or Pacific Islander\", \"Black or African American\", \"Other Races and Unknown combined\",\"White\"))     \n",
    "    genderlist =pd.Series(data=(\"Male\", \"Female\"))\n",
    "    \n",
    "    state_dummies=pd.get_dummies(statelist)\n",
    "    race_dummies=pd.get_dummies(racelist)\n",
    "    gender_dummies=pd.get_dummies(genderlist)\n",
    "\n",
    "    state_dummies_select = state_dummies[state].values\n",
    "    race_dummies_select = race_dummies[race].values\n",
    "    gender_dummies_select = gender_dummies[gender].values\n",
    "\n",
    "    state_dummy = state_dummies_select.tolist()\n",
    "    race_dummy = race_dummies_select.tolist()\n",
    "    gender_dummy = gender_dummies_select.tolist()\n",
    "    \n",
    "    data_cancer = race_dummy + gender_dummy  + state_dummy\n",
    "\n",
    "    pollution_data = pd.read_csv(\"../Data/Data_for_Graphing/ARpollution.csv\")\n",
    "    pollution_data_clean= pollution_data[[\"State\", \"Year\", \"CO\", \"Lead\", \"NO2\", \"Ozone\", \"PM10\", \"PM2_5\", \"SO2\"]]\n",
    "\n",
    "    data_poll = pollution_data_clean.loc[(pollution_data_clean['Year']==year) & (pollution_data_clean[\"State\"]==state)]\n",
    "\n",
    "\n",
    "    data_pollution_dropped = data_poll.drop([\"State\", \"Year\"], axis = 1)\n",
    "    data_pollution = data_pollution_dropped.values[0]\n",
    "    data_pollution_list = data_pollution.tolist()\n",
    "\n",
    "    data = data_pollution_list + data_cancer   \n",
    "    \n",
    "    \n",
    "    return np.array(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Larynx': '0.35513141192495823%', 'Lung': '99.64486360549927%', 'Nasal': '3.0915197068992484e-07%', 'Trachea': '9.988538621707403e-14%'}\n"
     ]
    }
   ],
   "source": [
    "values = getNNvalues(\"AK\", \"White\", 1999.0, \"Male\")\n",
    "print(NNpredict(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.05 ,  0.   ,  0.   ,  0.054, 14.   ,  0.   ,  0.   ,  0.   ,\n",
       "        0.   ,  0.   ,  0.   ,  1.   ,  1.   ,  0.   ,  1.   ,  0.   ,\n",
       "        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "        0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,\n",
       "        0.   ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
